# NumerFacts: Evaluating Numerical Factuality in Large Language Models  

[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![License: CC BY 4.0](https://img.shields.io/badge/License-CC%20BY%204.0-blue.svg)](https://creativecommons.org/licenses/by/4.0/)

## ðŸ“Œ Overview  
NumerFacts is the first benchmark dataset designed to evaluate **numerical factuality** in large language models (LLMs). This study introduces a **question-answering** framework to systematically analyse how well pre-trained LLMs recall numerical facts without any enhancement techniques (e.g., retrieval-augmented generation or fine-tuning).

ðŸ“„ **Thesis:** [Final Version](./NumerFacts.pdf)  
ðŸ”— **GitHub Repository:** [NumerFacts](https://github.com/Mau-B-Silva/NumerFacts)

## ðŸš€ Key Contributions  
- **Novel benchmark dataset:** Over **3,900 numerical facts** across **eight domains**.  
- **Evaluation of six open-source LLMs** in a **zero-shot** setting.  
- **Quantitative assessment of LLM accuracy, domain variability, and numerical recall.**  
- **Findings:** Large LMs struggle with numerical precision, achieving only **27.14% exact matches** at best.

## ðŸ“‚ Dataset  
The **NumerFacts** dataset is structured as follows:


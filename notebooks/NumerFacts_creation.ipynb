{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Excel files: ['Art & Literature.xlsx', 'Demographics.xlsx', 'Entertainment.xlsx', 'Geography.xlsx', 'History.xlsx', 'Personalities.xlsx', 'Science.xlsx', 'Sports.xlsx']\n"
     ]
    }
   ],
   "source": [
    "# Files are in 'domains' folder in the project. \n",
    "folder_path = './domains'\n",
    "\n",
    "# Listing  all Excel files in the folder (each of the 8 domains still split by properties) \n",
    "excel_files = [\n",
    "    f for f in os.listdir(folder_path)\n",
    "    if f.endswith('.xlsx') or f.endswith('.xls')\n",
    "]\n",
    "print(\"Found Excel files:\", excel_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading each Excel file’s sheets (each property) into a nested dictionary\n",
    "all_files_data = {}\n",
    "\n",
    "for excel_file in excel_files:\n",
    "    full_path = os.path.join(folder_path, excel_file)\n",
    "    xls = pd.ExcelFile(full_path)\n",
    "    sheet_dict = {}\n",
    "    \n",
    "    for sheet_name in xls.sheet_names:\n",
    "        df = pd.read_excel(full_path, sheet_name=sheet_name)\n",
    "        sheet_dict[sheet_name] = df\n",
    "    \n",
    "    all_files_data[excel_file] = sheet_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing a weighted sampling of 500 rows per Excel file, to account for \"not as populated\" properties.\n",
    "final_dataframes = []\n",
    "\n",
    "for excel_file, sheets_dict in all_files_data.items():\n",
    "    # Calculating the total rows across all sheets\n",
    "    sheet_sizes = {s: len(df) for s, df in sheets_dict.items()}\n",
    "    total_rows = sum(sheet_sizes.values())\n",
    "    \n",
    "    # Determining sampling sizes (weighted by row count)\n",
    "    sample_sizes = {}\n",
    "    for sheet_name, size in sheet_sizes.items():\n",
    "        sample_sizes[sheet_name] = int(500 * size / total_rows)  # floor\n",
    "    \n",
    "    # Fix rounding to ensure total is exactly 500\n",
    "    allocated = sum(sample_sizes.values())\n",
    "    difference = 500 - allocated\n",
    "    if difference != 0:\n",
    "        # Identifying the largest sheet to fill up the remainder, if needed\n",
    "        largest_sheet = max(sheet_sizes, key=sheet_sizes.get)\n",
    "        sample_sizes[largest_sheet] += difference\n",
    "    \n",
    "    # Sampling from each sheet\n",
    "    file_samples = []\n",
    "    for sheet_name, df in sheets_dict.items():\n",
    "        n_sample = sample_sizes[sheet_name]\n",
    "        if n_sample > 0:\n",
    "            # Ensuring it doesn’t sample more than the dataframe length\n",
    "            n_sample = min(n_sample, len(df))\n",
    "            \n",
    "            # Adding a random_state for reproducibility. Use this if you want to achieve the exact same sample, otherwise remove the random_state.\n",
    "            sampled_df = df.sample(n=n_sample, replace=False, random_state=17)\n",
    "            \n",
    "            # Adding columns to track origin (which category and which property)\n",
    "            sampled_df['fileName'] = excel_file\n",
    "            sampled_df['sheetName'] = sheet_name\n",
    "            \n",
    "            file_samples.append(sampled_df)\n",
    "    \n",
    "    # Combining all subsets for this file\n",
    "    file_sampled_concatenated = pd.concat(file_samples, ignore_index=True)\n",
    "    final_dataframes.append(file_sampled_concatenated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking all 8 of the sampled datasets\n",
    "final_df = pd.concat(final_dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a rowID column (ordered 1, 2, 3, ...)\n",
    "final_df['rowID'] = range(1, len(final_df) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the dataframe \n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: question, dtype: object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One more check for uniqueness of questions, in case duplicates fell through the cracks during the previous cleaning process\n",
    "non_unique_questions = final_df['question'][final_df['question'].duplicated(keep=False)]\n",
    "non_unique_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove ALL of these instances of non-unique questions\n",
    "final_df = final_df[~final_df['question'].isin(non_unique_questions)]\n",
    "\n",
    "# Display the updated dataset and verify\n",
    "print(\"Remaining rows after removing non-unique questions:\", final_df.shape[0])\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a \"days_since_last_modified\" column, with reference date as the 1st of January, 2025\n",
    "\n",
    "# Converting dateModified to do this calculation\n",
    "df['dateModified'] = pd.to_datetime(df['dateModified'], dayfirst=True)\n",
    "\n",
    "# Define the reference date (1st January 2025)\n",
    "reference_date = pd.Timestamp('2025-01-01')\n",
    "\n",
    "# Calculate the \"days_since_last_modified\" column\n",
    "df['days_since_last_modified'] = (pd.Timestamp('2025-01-01') - df['dateModified']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only the important columns for analysis.\n",
    "final_df_clean = final_df[[\"rowID\",\"entity\",\"entityLabel\",\"category\",\"property\", \"propertyLabel\", \"value\", \"roundedValue\", \"unitLabel\",\"sitelinks\",\"dateModified\",\"days_since_last_modified\",\"entityType\",\"question\",\"fileName\",\"sheetName\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming 'category' as 'domain', for consistency\n",
    "final_df_clean = final_df_clean.rename(columns={\"category\":\"domain\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final DataFrame shape: (3932, 28)\n"
     ]
    }
   ],
   "source": [
    "# Confirming final shape \n",
    "# This should return about 4000 entries, as we have 8 Excel files (domains) with 500 random entries each. \n",
    "# If there are less, is because one domain didn't have enough.\n",
    "print(\"Final DataFrame shape:\", final_df_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving to an Excel file\n",
    "final_df_clean.to_csv('NumerFacts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the \"questions\" separately, to probe the models and avoid any sort of leak\n",
    "if 'question' in final_df_clean.columns:\n",
    "    final_df_clean[['question']].to_csv('NumerFacts_questions_only.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
